Welcome to Cloud architecture and security section. In this section, we'll first look at some of the definitions that are commonly used when designing applications in AWS. Then we'll look at seven best practices for architecting solutions in Amazon. This is based on a paper Dr. Werner Vogels presented few years ago. The seven best practices are built security at every layer. Don't fear constraints, leverage different storage options in AWS. Implement elasticity. Loose coupling sets you free, designed for failure and nothing fails. Battle will go with each of these practices in detail in the module. So let's start with some definitions. You might have heard about some of these definitions in your prayer carrier, but we just thought we'll include it here for the sake of completeness. The first definition we want to talk about these RPO recovery point, objective. Rpo measures the maximum allowable data loss, often an outage, and this is measured in time. The next definition is RTO of recovery time objective. This defines the maximum tolerable length of time your system can be down after a disaster or a failure. These outage can be with a planned or unplanned outage. So you want your application is allowed to have only x minutes of downtime to the year. The next definition we want to talk about asymmetric encryption. Encryption technology, where the same key is used to encrypt as well as decrypt the data. So this is another 10. Another word for symmetric encryption is also known as also known as shared secret key. The DES algorithm are the three DES algorithm is an example of symmetric encryption. The next definition we want to talk about is asymmetric encryption. In asymmetric encryption, we use an EU. The next definition we want to talk about is asymmetric encryption. Asymmetric encryption a pair of related, a pair of mathematically related key pairs are used to encrypt the data. And the other one is to decrypt the data. So the SSH key pair that we use throughout this course uses is an example of asymmetric encryption. Encryption at rest. On the other hand, encryption on I dressed defines that the tools or the mechanism that we use to encrypt your data during storage on our disk. It also includes mechanism of access control, how who has access to the data, what mechanisms they need to access the data. Encryption in transit defines how we, how we want to encrypt the data during transmission. So secure protocol, protocol such as HTTPS, are used to encrypt data between endpoints. The next definition we want to understand is horizontal versus vertical scaling. In horizontal scaling, we add instances to support the workloads we add. We add more instances to support the workload. In other words, we might add more ec2 instances behind a load balancer to support the increased workload. Vertical scaling, on the other hand, use ads, either more CPU or more memory or more storage to the same physical server to make that server more powerful so that it's able to handle the workload. So when we go to Cloud services such as Amazon, we want to think horizontal scaling as much as we can when we're designing our application. The next definition we want to address is synchronous versus asynchronous replication. In synchronous replication, we record the data at two or more location for each right, so that the data always stays current. So the multi AZ deployment for RDS uses synchronous replication. So the asynchronous replication AR, we record data at the primary location and then copy it to the alternate location or the second location during scheduled intervals. So the next definition we want to talk about is fault tolerance. Fault tolerance is the ability of our system to continue operating in the event of failure of one or more components. For instance, when we use raid devices, redundant array of independent devices to construct our EBS volumes, we are using fault tolerance. In other words, if one or more disk fails in our array, our data is still accessible. We use multiple instances in case one instance fails or one availability zone goes away, our application is still able to sustain its still able to work under. Under that workload. Our application is still able to perform. We use multiple regions for the same reason. And we also talked about Route 53 failover. If one of the application or one of the region goes away, we are still able to serve the traffic to our users because we are using one of the fault tolerance mechanisms that's provided in a route 53 or high availability, on the other hand, is is the mechanism used to design systems that minimize downtime as much as possible. So the gold standard. When we're designing application is five-ninths on ninety-nine point nine, nine, nine percent. So when we design applications with five, nines of availability, we are essentially promising that the system will be available only for fire and a half minutes throughout the year. So the next definition we want to understand is eventual consistency versus strong consistency. So if you're talking about a distributed system like Amazon S3 for example, we know it makes multiple copies of a data. So when we would write something to S3, can be immediately read it and get consistent data because it's possible that all the data has not made it. So it's possible that the data has not made it to all the availability zones. And if you tried to read immediately after, right, we might get inconsistent data. So the eventual consistency allows a small lag time in fractions of a millisecond between the time the data is committed to all nodes. So this is something that we need to keep in mind when we're, when we're designing applications in AWS. The strong consistency or read after write consistency allows no lag, no time lag between the right. So in other words, it guarantees immediate visibility of the data. Let's switch gears and talk about the seven architectural best practices as recommended by Amazon. The first-principle we want to talk about is designed for failure and nothing fails. So when we're designing applications in AWS, we want to assume everything will fail. You want to be pessimistic and want to design application so that in the event of a failure, how will our application continue to respond? We want to deploy applications and databases in several availability zones in several regions. We want to make sure you take a regular snapshots of our EBS volumes. We want to anticipate failures at every single layer of our applications. We want to utilize RDS, automated backups. So in case of there is a failure, we'll be able to recolor to an ultimate side. We use multi AZ for the same, same principles. And we also want to use EBS back instances in a instead of the instance backed storage that we talked extensively when in the EC2 module. And we, if we have a custom AMI, we want to make sure we take regular backups of those AMIs. We want to make additional copies of those AMIs in the region. We want to use Route 53 DNS failover, big too, when you are designing applications. In AWS, we want to use multiple ELB is under Route 53, ELB per region. If everything fails, we want to have a last resort of S3, static website hosting. So S3 will at least provide an error page or an information to your users. Hey, the site is down, we're working to fix it, or at least they're aware of what's going on. We want to use auto-scaling to have minimum number of instances. So autoscaling does not have to, does not have to add more capacity to your applications. You can have auto-scaling to say I want to have only two instances at any given point. And auto-scaling, autoscaling will ensure you have at least two in two instances. Because that's your definition. We want to add multiple EC2 instances behind the load balancers. And we want to design the applications so that they are impervious to reboots and restarts. So that's the first-principle of architectural architecting in AWS designed for failure and nothing fails. So the second principle we want to talk about is called implement elasticity. That's the second principle recommended, recommended by Amazon. Now there are different types of elasticity. We can be proactive. You can, in other words, you know, there's the end of the month or the end of the quarter, there is an additional workload that gets put on your application. So you can scale based on that event or that time of the month. It can be event-based, the traffic searches, and you want to take action based on an event that's occurring in your application. It can be on-demand. You can automatically add and remove capacity as needed. So there are several types of elasticity we need to address. When we are designing for elasticity, we also have to figure out, do you want what, what type of AMI's do you want? Do you want an AMI that is bare bones and you configure it during the bootstrap method. Do you want to use an AMI that has pre-configured software that is already ready to go. So there are pros and cons of it with either approach. So you want to make sure that the method that you choose suits your application. You also want to design the applications with loose coupling. We'll talk about the coupling in a second. But essentially, coupling implies the degree of direct knowledge that one component has of another in the application. We want to minimize the coupling between these two, between multiple components in your application. So the goal is to reduce the risk that a change made to one component will cause unanticipated changes in other components. We also want to implement elasticity based on certain condition that are occurring in our environment. So we want to monitor system metrics to perform event-based elasticities. So during the auto-scaling section, we saw how we are monitoring the CPU utilization of our UAV for instances, and we were performing a last scaling actions based on that event. We are using auto-scaling to expand and shrink our capacity. We are also, we can also use tools like puppet or shelf to automatically build systems. I doing as part of elasticity principles. The third principle we want to talk about is loose coupling sets you free. We already talked about coupling a second ago, but coupling is the degree of knowledge one component has of another. So the goal is to reduce the change or reduce the risk. So the goal is to reduce the risk that a change made to one component will cause unanticipated changes in others. So we want to use APIs as much as possible. We want to treat each component as a black box. We want to use APIs to interact with these components. And we want to think, we want to design applications that are stateless building blocks. In other words, you provide an input to the block and you get an output back from the block. And this component in the middle could be replaced anytime as long as it performs. Are those long, as, as long as it behaves the same way. The loose coupling sets you free principle helps us reduce dependencies between components. We can use SQS, Simple Queue Service to bass messages between the components so that this dependencies gets reduced. We'll see an example of this in the next slide. So in this slide, we have a video encoding service. The users upload our videos. We store them in a location either it's S3 or a shared file storage. We encode the, the videos and then we publish that video. So as you can imagine, each component depends on the previous component. So and it has to go in a sequential fashion if there is an issue with, let's say in the encoding phase than the entire cycle gets broken here. To avoid that. So the better approach would be to use a cubist video and coding. So at the same example, but the users, in this case upload the videos and the message gets sent to SQS. The process, the upload process that takes the video, looks in the message, bulls the video and stores it in its local storage. And then it sends a message back to the encoding process, which then picks up the job and God said, and then when it's ready to be published, it sends a message back. As it sends a message to the publishing process using SQS. So as you can imagine, it's the same components. But now instead of having a direct communication between each of these components, we are sending messages back and forth using SQS. Now, it's, as you can imagine, this process is now easy to scale, easy, easy to maintain. If I need to scale my encoding services processes, I can just scale this part based on the number of jobs that are in this queue. So the next principle we want to talk about. Next best practice we want to talk about is build security at every layer. So Amazon uses a model called shared security model. So in this model, Amazon takes care of the physical network, datacenter and virtualization layer security. And the customers are responsible for the operating system, creating the users, creating the firewalls, the security groups, VPCs and so on. So AWS has lot of compliance requirement there metal out of compliance requirements, specifically FED ramp. They are HIPAA compliant applications can be hosted in AWS, the PCI DSS. In other words, you can accept credit cards by application hosted in AWS. They are ISO to 27, 1001 certified, their SAS 70 certified FISMA. I taught FIPS 140. So lot of these federal government certifications are, are acquired by Amazon. The next, the next best practice we want to talk about is think battle. So as you can imagine, parallelization is easy in Cloud because of the massive scalability that's offered by services like Amazon. It can be, it can run multi-threaded and concurrent request for you replicate from your applications. So services like Hadoop and ES slash EMR surveys lets you have one master and multiple slave nodes to how to run applications in parallel. We can also use ELB to spread the load across multiple instances. We can use Route 53 to spread load across multiple regions. To demonstrate the thing parallel is an application, a simple application that it's built in AWS. It uses AWS SQS to pass messages back and forth. And this application is requesting spot instances based on the number of jobs that are that need to be processed, and when the jobs are processed, the results are written in the process. Q. And if there is an exception, the messages are sent through an SQS, SQS queue called exception q. So this is how we can design parallel processing using SQS, Simple Queue Service. The next best practice we want to talk about is leverage different storage options in AWS. As we've seen throughout the class, Amazon offer several storage services such as EBS, S3, Glacier, and so on. So each, each storage has a different purpose. It has different characteristics, the scalability, cost, and backup options. As an architect, we need to evaluate the pros and cons of each storage when we choose application. So we need to understand the use case for the storage before we recommend or use any of these storage options. So for we also need to understand the requirement, the storage requirements for our RDBMS systems IN for our NoSQL databases. We also need to understand the security and encryption considerations of Story of these storage services. That are the other aspects we need to understand are the backup options and how to do long-term retentions if the application requires it. And if we can use instance store for our applications as you all, as we all learned, instance store is not meant to be. It's not meant for persistent data as we want. But there is, it's a free storage and it's much faster storage, even faster than the EBS storage. So we want to make sure that we use this judiciously. We need to understand the IOPS and the Pi obst options that are offered by IEP services like EBS. And the one principle we want to remember is we want to keep the dynamic data closer to date, closer to EC2. In other words, the data that's constantly changing should be closer to EC2. And static content such as images, the videos, the photos. That needs to be closer to the user using services like CloudFront. So depending on the type of objects or data we're storing. So for instance, we have large write once read many objects, some sort of content distribution, some sort of media, media usage, then S3 plus CloudFront would be a good use case for that one. If you're looking for scratch space, unimportant log files, arrow intermediate results. Then EC2 instance store might be a good option for that one. If you're looking for persistent storage, if you're looking for a database volumes website content than EBS volumes would be a better use case for that one. If you're looking for structure or transactional data, than RDS would be a good option for that one. Looking for a data warehousing type solution for analyzing your data, then redshift might be a better option for them. So as you can imagine, depending on the type of data, type, depending on type of the applications, we have different options that we can leverage in AWS. Finally, we want to talk about the last principle. Don't fear constraints. So constraints are easier to deal in Cloud rather than in traditional data centers because we have unlimited scalability. A lot of options for elasticity. We a lot of options for keeping our applications highly available. And so our design choices are, are a lot higher than in traditional data centers. We have lot of monitoring tools, lot of measurement tools to use it to identify our bottlenecks and take corrective actions. It's easier to design throwaway systems in our four what-if scenarios in AWS or any Cloud provider rather than in compared to traditional datacenters. So in summary, we learned some of the definitions that we need to be aware of when we're designing applications in AWS. We went to the seven architectural best practices, which are build security at every layer. Don't fear constraints, leverage different storage options. Implement elasticity. Loose coupling sets you free, designed for failure and nothing fails. And think paddle. 